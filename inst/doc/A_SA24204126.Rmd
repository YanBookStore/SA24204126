---
title: "A_SA24204126"
author: "SA24204126"
date: "2024-12-09"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{A_SA24204126}
  %\VignetteEncoding{UTF-8}
---

##  Question:Use knitr to produce at least 3 examples. For each example, texts should mix with figures and/or tables. Better to have mathematical formulas.

## Answer 0
## 示例 1: 图形和数学公式
```{r, echo=TRUE, fig.height=4, fig.width=6}
# 生成正态分布数据
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
plot(x, y, main="Scatter Plot of Normally Distributed Data", xlab="x", ylab="y", col="blue", pch=19)

# 添加图例
legend("topleft", legend = "Data Points", col = "blue", pch = 19)

# 在图中添加数学公式
text(x = 0, y = max(y) - 1, labels = expression(y == beta[0] + beta[1] * x + epsilon), cex = 1.2)

```

## 示例 2: 图表
```{r}

# 创建数据框
students <- data.frame(
  Name = c("Alice", "Bob", "Charlie", "David", "Eva"),
  Math = c(95, 78, 88, 92, 85),
  English = c(80, 85, 82, 88, 90),
  Physics = c(89, 90, 78, 95, 85),
  Chemistry = c(92, 88, 91, 84, 86),
  History = c(88, 94, 85, 90, 82)
)

# 显示数据框为表格
knitr::kable(students, caption = "学生考试成绩")

# 计算每个科目的均值、方差和标准差
stats <- data.frame(
  Subject = c("Math", "English", "Physics", "Chemistry", "History"),
  Mean = sapply(students[,-1], mean),
  Variance = sapply(students[,-1], var),
  SD = sapply(students[,-1], sd)
)

# 显示描述性统计量表格
knitr::kable(stats, caption = "各科目描述性统计量")

# 计算置信区间
confidence_intervals <- data.frame(
  Subject = stats$Subject,
  Lower = stats$Mean - 1.96 * stats$SD / sqrt(nrow(students)),
  Upper = stats$Mean + 1.96 * stats$SD / sqrt(nrow(students))
)

# 显示置信区间表格
knitr::kable(confidence_intervals, caption = "各科目均值的 95% 置信区间")

```


## 示例 3:正太分布曲线
```{r}
# 生成数据
x <- seq(-10, 10, length.out = 100)
y1 <- dnorm(x, mean = 0, sd = 1)
y2 <- dnorm(x, mean = 1, sd = 2)

# 绘制正态分布曲线
plot(x, y1, type = "l", col = "blue", lwd = 2, main = "Normal Distributions with Different Standard Deviations")
lines(x, y2, col = "red", lwd = 2)
legend("topright", legend = c("mean = 0,SD = 1", "mean = 1,SD = 2"), col = c("blue", "red"), lwd = 2)

```

## Answer 1
## 3.4 题目描述

Rayleigh分布密度函数是：

\[
f(x) = \frac{x}{\sigma^2} e^{-x^2 / (2\sigma^2)}, \quad x \geq 0, \sigma > 0.
\]

设计一个算法来生成来自Rayleigh分布 \(Rayleigh(\sigma)\) 的随机样本。为多个 \( \sigma > 0 \) 的值生成Rayleigh分布样本，并检查生成样本的众数是否接近理论众数 \( \sigma \)（检查直方图）。

### 解答
Rayleigh分布的概率密度函数为

$$
f(x) = \frac{x}{\sigma^2} e^{-x^2 / (2\sigma^2)}, \quad x \geq 0, \sigma > 0.
$$
我们将通过反函数变换法生成Rayleigh分布:
具体步骤如下：

1. 首先生成一个在 \( (0, 1) \) 区间上的均匀分布随机变量 \( U \sim Uniform(0, 1) \)。
2. 根据目标分布的累积分布函数（CDF）的反函数，将均匀分布的随机变量 \( U \) 转换为服从目标分布的随机变量。

在Rayleigh分布的情况下，累积分布函数 \(F(x)\) 为：

$$
F(x) = 1 - \exp\left(-\frac{x^2}{2\sigma^2}\right), \quad x \geq 0.
$$

然后将 \(F(x)\) 反解得出 \(x\)：

$$
x = \sigma \sqrt{-2 \log(1 - U)}.
$$

由于 \( U \) 是在 \( (0, 1) \) 上均匀分布，因此 \(1 - U\) 仍然是均匀分布随机变量。为了简化计算，可以直接使用：

$$
x = \sigma \sqrt{-2 \log(U)},
$$
```{r}
# 设定参数
set.seed(123)  # 设置随机数种子
n <- 10000     # 样本数量
sigma_values <- c(1, 2, 3)  # 不同的sigma值

# 定义Rayleigh分布的随机样本生成函数
rayleigh_sample <- function(sigma, n) {
  u <- runif(n)  # 生成n个均匀分布的随机数
  return(sigma * sqrt(-2 * log(u)))
}

# Rayleigh分布理论密度函数
rayleigh_density <- function(x, sigma) {
  (x / sigma^2) * exp(-x^2 / (2 * sigma^2))
}

# 生成样本并绘制直方图与理论密度函数比较
par(mfrow=c(1,3))  # 设置多图布局

# 设置不同sigma对应的y轴限制
y_limits <- list(c(0, 0.8), c(0, 0.4), c(0, 0.3))

for (i in 1:length(sigma_values)) {
  sigma <- sigma_values[i]
  samples <- rayleigh_sample(sigma, n)  # 生成Rayleigh分布样本
  
  # 绘制直方图
  hist(samples, breaks=50, probability=TRUE, main=paste("sigma =", sigma),
       xlab="样本值", col="lightblue", border="black", ylim=y_limits[[i]])
  
  # 生成x轴上的点用于绘制理论密度函数
  x_vals <- seq(0, max(samples), length=1000)
  
  # 绘制理论密度函数曲线
  lines(x_vals, rayleigh_density(x_vals, sigma), col="red", lwd=2)
  
  # 添加理论众数sigma的垂直线
  abline(v=sigma, col="blue", lwd=2, lty=2)  
  
  # 添加生成样本的实际众数
  actual_mode <- density(samples)$x[which.max(density(samples)$y)]
  abline(v=actual_mode, col="green", lwd=2, lty=3)
  
  # 添加图例
  legend("topright", legend=c("理论密度函数", "理论众数", "样本众数"),
         col=c("red", "blue", "green"), lwd=2, lty=c(1, 2, 3), cex=0.8)
}
```



## 3.11 问题描述

生成一个大小为1000的样本，该样本来自正态位置混合模型。混合模型的组成部分为 \(N(0, 1)\) 和 \(N(3, 1)\) 分布，混合概率分别为 \(p_1\) 和 \(p_2 = 1 - p_1\)。对 \(p_1 = 0.75\) 的情况绘制样本的直方图并叠加密度图。然后使用不同的 \(p_1\) 值重复实验，观察混合模型的经验分布是否呈现双峰形态，并提出关于双峰混合模型的猜想。

### 解答
### R 代码

```{r fig.width=7, fig.height=5}

set.seed(123)  # 保证可重复性

# 样本大小
n <- 1000

# 混合分布的生成函数
generate_mixture <- function(p1, n) {
  # 生成混合分布
  r <- rbinom(n, 1, p1)
  # r 为1时从 N(0,1) 生成，否则从 N(3,1) 生成
  samples <- rnorm(n, mean = ifelse(r == 1, 0, 3), sd = 1)
  return(samples)
}

# 设置混合概率 p1 = 0.75
p1 <- 0.75
samples <- generate_mixture(p1, n)

# 基础R绘图 - 绘制直方图并叠加密度图
hist(samples, probability = TRUE, breaks = 30, 
     main = paste("p1 =", p1, "时的样本分布"), xlab = "样本值", col = "lightblue")

# 添加密度曲线
lines(density(samples), col = "red", lwd = 2)
# 设置不同的 p1 值
p_values <- c(0.1, 0.25, 0.5, 0.75, 0.9)

# 绘制不同 p1 值下的直方图
par(mfrow = c(3, 2))  # 设置图形布局

for (p1 in p_values) {
  samples <- generate_mixture(p1, n)
  
  hist(samples, probability = TRUE, breaks = 30, 
       main = paste("p1 =", p1), xlab = "样本值", col = "lightblue")
  
  lines(density(samples), col = "red", lwd = 2)
}
par(mfrow = c(1, 1))  # 恢复单一图形布局


```
### 结果与讨论

通过观察不同 \(p_1\) 值下的分布形状，我们可以得出以下结论：

- 当 \(p_1 = 0.1\) 或 \(p_1 = 0.9\) 时，样本分布的形状近似为单峰，接近于一个标准正态分布或偏移的正态分布。
- 当 \(p_1 = 0.5\) 时，混合分布的形状呈现出明显的双峰特性，这说明在该情况下，两个不同的正态分布对整体的贡献接近相等，从而导致了双峰现象。

## 3.20 题目描述

复合泊松过程是一种随机过程 \(\{X(t), t \geq 0\}\)，可以表示为随机和 \(X(t) = \sum_{i=1}^{N(t)} Y_i\)，其中 \(t \geq 0\)，\(N(t), t \geq 0\) 是一个泊松过程，且 \(Y_1, Y_2, \dots\) 是独立同分布的，且与 \(\{N(t), t \geq 0\}\) 独立。编写一个程序来模拟复合泊松 \(\lambda\) - 伽马过程（其中 \(Y\) 服从伽马分布）。估计 \(X(10)\) 的均值和方差，针对不同参数进行多次选择，并与理论值进行比较。提示：证明 \(E[X(t)] = \lambda t E[Y_1]\) 和 \(Var(X(t)) = \lambda t E[Y_1^2]\)。

### 解答
### 理论背景

- 均值: \( E[X(t)] = \lambda t E[Y_1] \)
- 方差: \( \text{Var}(X(t)) = \lambda t E[Y_1^2] \)

伽马分布的期望和方差为:
- \( E[Y_1] = \alpha / \beta \)
- \( \text{Var}(Y_1) = \alpha / \beta^2 \)

因此，复合泊松过程的均值和方差为:
- \( E[X(t)] = \lambda t \cdot (\alpha / \beta) \)
- \( \text{Var}(X(t)) = \lambda t \cdot \left(\frac{\alpha}{\beta^2} + \left(\frac{\alpha}{\beta}\right)^2 \right) \)

### 模拟过程

```{r, warning=FALSE}
# 加载必要的库
set.seed(123)
lambda <- 2  # 泊松过程的参数
t <- 10      # 时间点 t
alpha <- 3   # 伽马分布参数
beta <- 2    # 伽马分布参数

# 模拟复合泊松-伽马过程
simulate_compound_poisson_gamma <- function(lambda, alpha, beta, t) {
  N_t <- rpois(1, lambda * t)  # 泊松过程模拟 N(t)
  Y <- rgamma(N_t, shape = alpha, rate = beta)  # 伽马分布模拟 Y_i
  X_t <- sum(Y)  # 复合泊松过程的和
  return(X_t)
}

# 进行多次模拟，计算均值和方差
n_simulations <- 10000
results <- replicate(n_simulations, simulate_compound_poisson_gamma(lambda, alpha, beta, t))

# 估计均值和方差
estimated_mean <- mean(results)
estimated_variance <- var(results)

# 理论均值和方差
theoretical_mean <- lambda * t * (alpha / beta)
theoretical_variance <- lambda * t * (alpha / beta^2 + (alpha / beta)^2)

# 显示结果
estimated_mean
estimated_variance
theoretical_mean
theoretical_variance
```

### 结果

模拟的均值和方差，以及理论的均值和方差如下：

- 模拟均值: 30.14362
- 模拟方差: 59.97725
- 理论均值: 30
- 理论方差: 60

可以看出，模拟值与理论值非常接近，证明了复合泊松-伽马过程的模型是正确的。

### 结论

通过本次实验，我们成功模拟了复合泊松-伽马过程，并验证了理论的均值和方差。模拟结果与理论推导的值非常接近，证明了该模拟方法的有效性。

## Answer 2

## 5.4 Monte Carlo estimate
Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,
and use the function to estimate F(x) for x = 0.1, 0.2,..., 0.9. Compare the
estimates with the values returned by the pbeta function in R

## 解
对于参数为 $\alpha$ 和 $\beta$ 的 Beta 分布，累积分布函数 $F(x)$ 在区间 $[0, 1]$ 上定义为：

$$
F(x; \alpha, \beta) = \int_0^x \frac{t^{\alpha - 1} (1 - t)^{\beta - 1}}{B(\alpha, \beta)} dt
$$

其中：

- $\alpha, \beta > 0$ 是 Beta 分布的两个参数，
- $B(\alpha, \beta)$ 是 Beta 函数，它可以表示为 $\Gamma$ 函数的比值：

$$
B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
$$
对于 Beta(3, 3) 分布，其累积分布函数为：

$$
F(x; 3, 3) = \int_0^x \frac{t^2 (1 - t)^2}{B(3, 3)} dt
$$

$$
B(3, 3) = \frac{\Gamma(3) \Gamma(3)}{\Gamma(6)} = \frac{2! \cdot 2!}{5!} = \frac{4}{120} = \frac{1}{30}
$$

因此，Beta(3, 3) 分布的累积分布函数是：

$$
F(x; 3, 3) = 30 \int_0^x t^2 (1 - t)^2 dt
$$

下面通过蒙特卡洛方法进行估计该分布函数:

$$
F(x; 3 , 3) = 30\int_0^x t^2 (1 - t)^2 dt = 30\int_0^x x\cdot t^2 (1 - t)^2 \frac{1}{x-0}dt = 30E_Y[xY^2(1-Y)^2] \qquad Y \sim U(0,x)
$$

代码实现如下：

```{r}

Monte_Carlo_Beta33 <- function(x=0.1,n=1e6){
  y <- runif(n,min = 0, max = x)
  result <- 30 * mean(x * y*y * (1-y)^2)
  return(result)
}

for (x in seq(0.1, 0.9, by = 0.1)) {
  print(paste("x=",x,"Monte Carlo method:", round(Monte_Carlo_Beta33(x),digits = 5) , "pbeta function:" , pbeta(x, shape1 = 3, shape2 = 3)))
}

```





# 5.9 Rayleigh Density and Variance Reduction

The Rayleigh density [156, (18.76)] is given by

$$
f(x) = \frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)}, \quad x \geq 0, \sigma > 0.
$$
We are tasked with implementing a function to generate samples from a Rayleigh($\sigma$) distribution using **antithetic variables**. The goal is to explore the **percent reduction in variance** of the estimator$\frac{X + X'}{2}$
compared to the variance of$\frac{X_1 + X_2}{2}$for independent $X_1$ and $X_2$.


# 解
Rayleigh 分布的概率密度函数如下：

$$
f(x) = \frac{x}{\sigma^2} e^{-x^2 / (2\sigma^2)}, \quad x \geq 0, \sigma > 0.
$$

根据反函数法，求得
$$
X = \sigma\sqrt{-2 * log(u)}\quad u\sim U(0,1)
$$
据此定义一个从 Rayleigh 分布中生成样本的函数。

```{r}
# 从 Rayleigh 分布中生成样本
rayleigh_sample <- function(sigma, n) {
  u <- runif(n)
  return(sigma * sqrt(-2 * log(u)))
}
```

## 使用对偶变量生成样本

接下来，我们使用对偶变量（antithetic variables）生成样本。对偶变量的方法通过减少随机数的波动来减少方差。

```{r}
rayleigh_antithetic <- function(sigma, n) {
  u <- runif(n/2)
  x1 <- sigma * sqrt(-2 * log(u))
  x2 <- sigma * sqrt(-2 * log(1 - u))
  return(c(x1, x2))
}
```

## 计算方差

我们将生成两个样本集，一个使用独立样本，一个使用对偶变量。接着，我们比较方差并计算方差的减少百分比。

```{r}
# 参数设置
sigma <- 1
n <- 10000

# 独立样本
x1 <- rayleigh_sample(sigma, n)
x2 <- rayleigh_sample(sigma, n)
independent_mean <- (x1 + x2) / 2

# 对偶变量
x_antithetic <- rayleigh_antithetic(sigma, n)
antithetic_mean <- (x_antithetic[1:(n/2)] + x_antithetic[(n/2 + 1):n]) / 2

# 计算方差
var_independent <- var(independent_mean)
var_antithetic <- var(antithetic_mean)

# 计算方差减少百分比
percent_reduction <- (var_independent - var_antithetic) / var_independent * 100

cat("独立样本的方差:", var_independent, "\n")
cat("对偶变量的方差:", var_antithetic, "\n")
cat("方差减少百分比:", percent_reduction, "%\n")
```

## 结果

通过上述代码，我们得出了独立样本与对偶变量的方差。对偶变量的方差较小，且方差减少的百分比如下。

```{r}
# 打印结果
percent_reduction
```


# 5.13
Find two importance functions \( f_1 \) and \( f_2 \) that are supported on \( (1, \infty) \) and are "close" to

\[
g(x) = \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2}, \quad x > 1.
\]

Which of your two importance functions should produce the smaller variance in estimating

\[
\int_1^\infty \frac{x^2}{\sqrt{2\pi}} e^{-x^2 / 2} \, dx
\]

by importance sampling? Explain.


## 解

考虑以下两个重要性函数：

$f_1(x)$：我们对标准正态分布的$1到\infty$的分布进行归一化处理，作为第一个重要性函数

$$
f_1(x) = \frac{\frac{1}{\sqrt{2\pi}} e^{-x^2/2}}{\int_1^\infty \frac{1}{\sqrt{2\pi}} e^{-s^2/2}ds}, \quad x > 1.
$$
 其中,$\int_1^\infty \frac{1}{\sqrt{2\pi}} e^{-s^2/2}ds$大约为0.1587，因此有
$$
 f_1(x) = \frac{\frac{1}{\sqrt{2\pi}} e^{-x^2/2}}{0.1587}, \quad x > 1.
$$
 

$f_2(x)$：另一种选择是选用$x$的幂次分布
$$
  f_2(x) = \frac{2}{x^3}, \quad x > 1.
$$
该分布的权重在 \( x \) 较大时衰减得较慢。


```{r}
set.seed(123)

# 目标函数 g(x)
g <- function(x) {
  return((x^2 / sqrt(2 * pi)) * exp(-x^2 / 2))
}

# 重要性函数 f1(x): 截断的正态分布
f1_density <- function(x) {
  return(dnorm(x) / (1 - pnorm(1)))
}

# 重要性函数 f2(x): 幂分布
f2_density <- function(x) {
  return(2 / x^3)
}

# 样本大小
N <- 10000

# 使用 f1 进行重要性采样
f1_samples <- rnorm(N)
f1_samples <- f1_samples[f1_samples > 1]  # 只取大于1的部分
weights_f1 <- g(f1_samples) / f1_density(f1_samples)
I_f1 <- mean(weights_f1)
var_f1 <- var(weights_f1)

# 使用 f2 进行重要性采样
f2_samples <- (1 + rexp(N, 1))  # 对于幂分布的采样
weights_f2 <- g(f2_samples) / f2_density(f2_samples)
I_f2 <- mean(weights_f2)
var_f2 <- var(weights_f2)

# 输出结果
cat("f1 估计值: ", I_f1, "\n")
cat("f1 方差: ", var_f1, "\n")
cat("f2 估计值: ", I_f2, "\n")
cat("f2 方差: ", var_f2, "\n")

```

两种方法中$f_1$函数的方差更小。



# Monte Carlo experiment

- For \( n = 10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4 \), apply the fast sorting algorithm to randomly permuted numbers from \( 1, \ldots, n \).
- Calculate the computation time averaged over 100 simulations, denoted by \( a_n \).
- Regress \( a_n \) on \( t_n := n \log(n) \), and graphically show the results (scatter plot and regression line).


```{r}

# 设置n的值
n_values <- c(10^4, 2*10^4, 4*10^4, 6*10^4, 8*10^4)

# 创建存储平均时间的向量
mean_times <- numeric(length(n_values))

# 对每个n进行实验
for (i in 1:length(n_values)) {
  n <- n_values[i]
  times <- numeric(100)  # 存储每次的时间
  
  # 进行100次实验
  for (j in 1:100) {
    vec <- sample(1:n)  # 生成1到n的随机排列
    time_taken <- system.time(sort(vec))['elapsed']  # 测量排序的时间
    times[j] <- time_taken
  }
  
  # 计算平均时间
  mean_times[i] <- mean(times)
}

# 计算 n*log(n)
t_n <- n_values * log(n_values)

# 进行线性回归
regression_model <- lm(mean_times ~ t_n)

# 显示回归结果
summary(regression_model)

# 绘制散点图和回归线
plot(t_n, mean_times, main = "Sorting Time vs n log(n)", xlab = "n log(n)", ylab = "Mean Time (s)")
abline(regression_model, col="red")  # 添加回归线


```

## Answer 3

# 6.6

Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness \( \sqrt{b_1} \) under normality using a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation \( \sqrt{b_1} \approx N(0, 6/n) \).

\[
Var(\hat{x}_q) = \frac{q(1 - q)}{n f(x_q)^2} \quad (2.14)
\]

### 步骤1：蒙特卡罗模拟来估计 \(\sqrt{b_1}\) 的分位数
我们要进行大量的模拟，计算每个样本的偏度，并提取所需的分位数（0.025、0.05、0.95 和 0.975）。

### 步骤2：计算标准误差
我们将使用公式(2.14)来计算分位数的标准误差：

\[
Var(\hat{x}_q) = \frac{q(1 - q)}{n f(x_q)^2}
\]

这里，\(f(x_q)\) 是在分位数处的密度函数值。

### 步骤3：与大样本近似进行比较
大样本近似为 \(\sqrt{b_1} \approx N(0, \frac{6}{n})\)，我们需要比较模拟结果与这个近似结果。

#### R代码实现

```{r}
# 加载所需库
library(moments)  # 用于计算偏度

# 定义模拟参数
set.seed(123)
n <- 100  # 样本量
num_simulations <- 10000  # 蒙特卡罗模拟次数
quantiles <- c(0.025, 0.05, 0.95, 0.975)

# 存储模拟结果
skewness_sqrt_b1 <- numeric(num_simulations)

# 蒙特卡罗模拟
for (i in 1:num_simulations) {
  sample_data <- rnorm(n)  # 生成正态分布样本
  b1 <- skewness(sample_data)  # 计算偏度
  
  # 对负的偏度取绝对值
  skewness_sqrt_b1[i] <- sqrt(abs(b1))  # 保存偏度的绝对值的平方根
}

# 计算分位数
estimated_quantiles <- quantile(skewness_sqrt_b1, quantiles, na.rm = TRUE)
print(estimated_quantiles)

# 计算标准误差 (公式2.14)
# 正态分布的密度函数在每个分位数的值
density_values <- dnorm(qnorm(quantiles))
n_sample <- length(skewness_sqrt_b1)

# 公式(2.14)
standard_errors <- sqrt(quantiles * (1 - quantiles) / (n_sample * density_values^2))
print(standard_errors)

# 大样本近似
large_sample_approx <- sqrt(6/n) * qnorm(quantiles)
print(large_sample_approx)

# 输出比较结果
results <- data.frame(
  Quantile = quantiles,
  Estimated = estimated_quantiles,
  Standard_Error = standard_errors,
  Large_Sample_Approx = large_sample_approx
)

print(results)
```
# 6B
Tests for association based on Pearson product moment correlation \(\rho\), Spearman's rank correlation coefficient \(\rho_s\), or Kendall's \(\tau\), are implemented in `cor.test`. Show (empirically) that the nonparametric tests based on \(\rho_s\) or \(\tau\) are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate distribution \(X, Y\) such that \(X\) and \(Y\) are dependent) such that at least one of the nonparametric tests has better empirical power than the correlation test against this alternative."


1. **实证展示：在双变量正态分布下，非参数检验的功效低于皮尔逊相关性检验**。我们可以通过生成双变量正态分布的数据并进行皮尔逊、斯皮尔曼、和肯德尔的相关性检验，然后计算每个检验的功效。

2. **寻找替代分布：找到一个 \( X \) 和 \( Y \) 之间有依赖关系的双变量分布，使得非参数检验（斯皮尔曼或肯德尔）比皮尔逊检验具有更高的统计功效**。一种常见的选择是使用一种非线性关系。

### 1. 双变量正态分布情况下的功效比较

首先，我们在双变量正态分布的情况下比较皮尔逊、斯皮尔曼和肯德尔的功效。

```{r}
# 加载所需的库
set.seed(123)  # 固定随机种子
library(bootstrap)
library(coda)
library(MASS)  # 用于生成双变量正态分布
library(coin)  # 用于计算功效
library(survival)
# 生成双变量正态分布数据
n <- 100  # 样本大小
mu <- c(0, 0)  # 均值向量
Sigma <- matrix(c(1, 0.8, 0.8, 1), 2, 2)  # 协方差矩阵，相关系数为 0.8
bivariate_normal <- mvrnorm(n, mu, Sigma)

X <- bivariate_normal[, 1]
Y <- bivariate_normal[, 2]

# 皮尔逊相关检验
pearson_test <- cor.test(X, Y, method = "pearson")

# 斯皮尔曼相关检验
spearman_test <- cor.test(X, Y, method = "spearman")

# 肯德尔相关检验
kendall_test <- cor.test(X, Y, method = "kendall")

# 输出检验结果
pearson_test$p.value
spearman_test$p.value
kendall_test$p.value
```

在双变量正态分布的情况下，皮尔逊检验应该比斯皮尔曼和肯德尔检验具有更高的功效，特别是在样本量较小的情况下。

### 2. 找一个非正态的双变量分布，使得非参数检验的功效更高

为了找到一个替代分布，我们可以考虑使用非线性关系。例如，考虑以下几种分布：

- **Quadratic relationship (二次关系)**: \( Y = X^2 \)
- **Sine wave relationship (正弦关系)**: \( Y = \sin(X) \)

这些非线性关系可能会使得斯皮尔曼和肯德尔的非参数检验比皮尔逊的功效更高，因为皮尔逊假定线性关系。

```{r}
# 生成非线性关系的数据: Y = X^2
n <- 100  # 样本大小
X <- rnorm(n)
Y <- X^2 + rnorm(n, sd = 0.1)  # 加入一些噪声

# 皮尔逊相关检验
pearson_test_nonlin <- cor.test(X, Y, method = "pearson")

# 斯皮尔曼相关检验
spearman_test_nonlin <- cor.test(X, Y, method = "spearman")

# 肯德尔相关检验
kendall_test_nonlin <- cor.test(X, Y, method = "kendall")

# 输出非线性关系下的检验结果
pearson_test_nonlin$p.value
spearman_test_nonlin$p.value
kendall_test_nonlin$p.value
```
## 结论
在正态分布的线性关系下，皮尔逊检验比非参数检验更强。
在非线性关系下，斯皮尔曼和肯德尔的非参数检验比皮尔逊检验更强。


## 假设检验
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.**

- What is the corresponding hypothesis test problem?
- What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?
- Please provide the least necessary information for hypothesis testing.


## 假设检验问题

该问题可以表述为双样本假设检验，假设如下：

- **原假设 (H₀)：** 两种方法的功效相同，即 \( \mu_1 = \mu_2 \)。
- **备择假设 (H₁)：** 两种方法的功效不同，即 \( \mu_1 \neq \mu_2 \)。

## 检验方法选择

我们将使用**双样本Z检验**来比较两种方法的功效。选择该检验的原因是我们比较的是两个独立组的比例，并且由于实验次数较多（10,000次），可以使用正态近似（Z检验）。

t检验（如两样本t检验或配对t检验）通常用于比较均值，而不是比例；McNemar检验则用于配对的名义数据，因此不适合该问题。

## 假设检验的必要信息

- 样本量：每种方法各10,000次实验。
- 比例（功效）：方法1为0.651，方法2为0.676。
- 显著性水平：0.05

## R代码：双样本Z检验

```{r}
# 给定的数据
n1 <- 10000  # 方法1的样本量
n2 <- 10000  # 方法2的样本量
p1 <- 0.651  # 方法1的功效
p2 <- 0.676  # 方法2的功效

# 组合后的比例
p_combined <- (p1 * n1 + p2 * n2) / (n1 + n2)

# 标准误差
se <- sqrt(p_combined * (1 - p_combined) * (1/n1 + 1/n2))

# Z统计量
z_stat <- (p1 - p2) / se

# P值
p_value <- 2 * (1 - pnorm(abs(z_stat)))

# 输出结果
z_stat
p_value

# 基于显著性水平0.05做出决策
if (p_value < 0.05) {
  cat("拒绝原假设：两种方法的功效显著不同。")
} else {
  cat("未能拒绝原假设：两种方法的功效没有显著差异。")
}
```

## Answer 4

## 题目
在 \( N = 1000 \) 个假设中，950 个是原假设（即空假设），50 个是备择假设。对于任意空假设下的 P 值是均匀分布的（使用 `runif` 生成），而对于备择假设下的 P 值服从参数为 0.1 和 1 的 Beta 分布（使用 `rbeta` 生成）。要求获得 Bonferroni 校正的 P 值和 B-H 校正的 P 值。在名义水平 \( \alpha = 0.1 \) 下，分别计算 FWER（家族错误率）、FDR（假发现率）和 TPR（真正率），并根据 \( m = 10000 \) 次模拟来获得结果。要求输出 6 个数值（3 个）到一个 \( 3 \times 2 \) 的表格中（列名为：Bonferroni 校正、B-H 校正；行名为：FWER, FDR, TPR）。最后，请对结果进行评论。

简而言之，这个问题要求你通过模拟计算两种不同校正方法（Bonferroni 和 B-H 校正）下的 FWER、FDR 和 TPR，并比较这些方法的表现。

## Setup

首先我们加载必要的库，并设置初始参数。

```{r}
# 设置随机种子，确保结果可重复
set.seed(123)

# 参数设置
N <- 1000       # 假设总数
m <- 10000      # 模拟次数
null_count <- 950  # 空假设的数量
alt_count <- 50    # 备择假设的数量
alpha <- 0.1    # 名义显著性水平

# 数据存储
results <- data.frame(matrix(numeric(6), nrow = 3, ncol = 2))
colnames(results) <- c("Bonferroni", "B-H")
rownames(results) <- c("FWER", "FDR", "TPR")

```

## P-value Simulation

接下来我们生成 P 值。对于空假设下的 P 值，使用 `runif()` 生成均匀分布的值；而对于备择假设下的 P 值，使用 `rbeta()` 生成 Beta 分布的值。

```{r}
# 函数用于生成一个实验中的p值
generate_pvalues <- function() {
  # 生成空假设的p值
  null_pvals <- runif(null_count)
  
  # 生成备择假设的p值
  alt_pvals <- rbeta(alt_count, 0.1, 1)
  
  # 合并p值
  pvals <- c(null_pvals, alt_pvals)
  return(pvals)
}

# 测试p值生成
pvals <- generate_pvalues()
head(pvals)
```

## Bonferroni 和 B-H 校正

我们通过应用 Bonferroni 校正和 Benjamini-Hochberg (B-H) 校正来调整 P 值，并计算每个校正方法下的 FWER、FDR 和 TPR。

```{r}
# Bonferroni 校正
bonferroni_adjust <- function(pvals) {
  return(p.adjust(pvals, method = "bonferroni"))
}

# B-H 校正
bh_adjust <- function(pvals) {
  return(p.adjust(pvals, method = "BH"))
}

# 计算 FWER, FDR, 和 TPR
calculate_metrics <- function(adjusted_pvals, true_alt_indices, alpha) {
  # 检测为显著的假设
  discoveries <- which(adjusted_pvals < alpha)
  
  # 计算 FWER (至少一个假阳性的概率)
  fwer <- ifelse(any(discoveries <= null_count), 1, 0)
  
  # 计算 FDR (假阳性率)
  false_discoveries <- sum(discoveries <= null_count)
  total_discoveries <- length(discoveries)
  fdr <- ifelse(total_discoveries > 0, false_discoveries / total_discoveries, 0)
  
  # 计算 TPR (真正率)
  true_discoveries <- sum(discoveries > null_count)
  tpr <- true_discoveries / alt_count
  
  return(c(fwer, fdr, tpr))
}
```

## 模拟实验

我们进行 \( m = 10000 \) 次模拟实验，分别计算 Bonferroni 校正和 B-H 校正下的 FWER, FDR 和 TPR。

```{r, message=FALSE, warning=FALSE}
# 存储 Bonferroni 和 B-H 校正结果的矩阵
bonferroni_results <- matrix(0, nrow = m, ncol = 3)
bh_results <- matrix(0, nrow = m, ncol = 3)

# 进行模拟
for (i in 1:m) {
  pvals <- generate_pvalues()
  
  # Bonferroni 校正
  bonf_pvals <- bonferroni_adjust(pvals)
  bonferroni_results[i, ] <- calculate_metrics(bonf_pvals, null_count, alpha)
  
  # B-H 校正
  bh_pvals <- bh_adjust(pvals)
  bh_results[i, ] <- calculate_metrics(bh_pvals, null_count, alpha)
}

# 计算平均值作为最终结果
results["FWER", "Bonferroni"] <- mean(bonferroni_results[, 1])
results["FDR", "Bonferroni"] <- mean(bonferroni_results[, 2])
results["TPR", "Bonferroni"] <- mean(bonferroni_results[, 3])

results["FWER", "B-H"] <- mean(bh_results[, 1])
results["FDR", "B-H"] <- mean(bh_results[, 2])
results["TPR", "B-H"] <- mean(bh_results[, 3])

# 输出结果表格
results
```

## 结果评论

从模拟结果来看，我们可以观察到 Bonferroni 校正和 B-H 校正在控制 FWER 和 FDR 方面的差异。

```{r}
# 评论结果
print("Bonferroni 校正相对保守，因此它会较好地控制 FWER，但可能导致较低的 TPR，即真正发现的比例较低。")
print("B-H 校正更倾向于控制 FDR，允许更多的假阳性，但可能提高 TPR。")
```

## 7.4
请参考 **boot** 包中提供的空调数据集 **aircondit**。这12个观测值表示空调设备故障之间的时间（单位为小时）[63, 示例 1.1]：

3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487。

假设故障之间的时间服从指数模型 **Exp(λ)**。求出失效率 **λ** 的**最大似然估计（MLE）**，并使用自助法（bootstrap）估计该估计值的偏差和标准误。



## 1. 数据

以下是代表空调设备故障时间（单位为小时）的 12 个观测值。

```{r}
# 空调设备故障时间数据（单位：小时）
failure_times <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
failure_times
```

## 2. 失效率 \( \lambda \) 的最大似然估计

假设数据遵循指数分布 \( \text{Exp}(\lambda) \)，则失效率 \( \lambda \) 的最大似然估计为：

\[
\hat{\lambda} = \frac{1}{\bar{x}}
\]

其中，\( \bar{x} \) 为观测数据的均值。

```{r mle_lambda}
# 计算 lambda 的最大似然估计
lambda_mle <- 1 / mean(failure_times)
lambda_mle
```

## 3. 自助法（Bootstrap）

自助法用于估计 \( \lambda \) 的偏差和标准误。每次重采样时重新计算 \( \lambda \) 的最大似然估计。

```{r bootstrap}
# 加载必要的包
library(boot)

# 定义用于 bootstrap 的统计函数
mle_stat <- function(data, indices) {
  resample_data <- data[indices]
  1 / mean(resample_data)
}

# 使用 1000 次重采样进行自助法
set.seed(123)
bootstrap_results <- boot(data = failure_times, statistic = mle_stat, R = 1000)

# 显示自助法结果
bootstrap_results
```

## 4. 偏差和标准误

通过 `boot` 函数的结果，我们可以提取 \( \lambda \) 的偏差和标准误。

```{r bias_se}
# 计算自助法得到的偏差和标准误
bias <- mean(bootstrap_results$t) - lambda_mle
standard_error <- sd(bootstrap_results$t)

bias
standard_error
```

## 5. 结论

失效率 \( \lambda \) 的最大似然估计为 `r lambda_mle`。使用自助法估计的偏差为 `r bias`，标准误为 `r standard_error`。

### 解释：

1. **数据部分**：定义并展示了空调设备故障时间数据。
2. **最大似然估计（MLE）**：根据公式 \( \hat{\lambda} = \frac{1}{\bar{x}} \)，计算失效率 \( \lambda \) 的最大似然估计。
3. **自助法（Bootstrap）**：使用 `boot` 包对数据进行1000次自助法重采样，计算每次重采样中的 \( \lambda \) 值。
4. **偏差与标准误**：通过自助法的结果，计算出估计值的偏差和标准误。


## 7.5
参考练习7.4。通过标准正态法、基本法、百分位法和BCa（偏差校正加速）法计算故障间隔时间 \(1/\lambda\) 的95%自助法置信区间。比较这些区间，并解释它们可能为什么会有所不同。


## 1. 数据

以下是代表空调设备故障时间（单位为小时）的 12 个观测值。

```{r data}
# 空调设备故障时间数据（单位：小时）
failure_times <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
failure_times
```

## 2. 计算 \( 1/\lambda \) 的最大似然估计

根据题目 7.4，最大似然估计 \( \hat{\lambda} \) 可以通过下式得到：

\[
\hat{\lambda} = \frac{1}{\bar{x}}
\]

那么，故障间隔时间 \( 1/\lambda \) 的估计值为 \( \bar{x} \)，即观测数据的均值。

```{r mle_mean_time}
# 计算平均故障间隔时间（即 1/lambda 的估计值）
mean_time <- mean(failure_times)
mean_time
```

## 3. 自助法置信区间

通过 `boot` 包对数据进行自助法，并使用标准正态法、基本法、百分位法和 BCa 法计算置信区间。

```{r bootstrap_intervals}
# 加载必要的包
library(boot)

# 定义统计函数，返回 1/lambda
mean_time_stat <- function(data, indices) {
  resample_data <- data[indices]
  mean(resample_data)
}

# 使用 1000 次重采样进行自助法
set.seed(123)
bootstrap_results <- boot(data = failure_times, statistic = mean_time_stat, R = 1000)

# 计算标准正态、基本、百分位和 BCa 置信区间
ci_norm <- boot.ci(bootstrap_results, type = "norm")
ci_basic <- boot.ci(bootstrap_results, type = "basic")
ci_perc <- boot.ci(bootstrap_results, type = "perc")
ci_bca <- boot.ci(bootstrap_results, type = "bca")

# 展示置信区间
ci_norm
ci_basic
ci_perc
ci_bca
```

## 4. 比较置信区间

让我们将所有四种方法得到的 95% 置信区间进行比较。

```{r compare_intervals}
# 将置信区间结果提取出来并汇总
ci_summary <- data.frame(
  Method = c("Standard Normal", "Basic", "Percentile", "BCa"),
  Lower = c(ci_norm$normal[2], ci_basic$basic[4], ci_perc$perc[4], ci_bca$bca[4]),
  Upper = c(ci_norm$normal[3], ci_basic$basic[5], ci_perc$perc[5], ci_bca$bca[5])
)

ci_summary
```

## 5. 解释置信区间的差异

置信区间之间的差异主要来自不同的方法假设和对偏差的校正：

- **标准正态法**：假设统计量是正态分布的，并使用标准误差来计算区间。这种方法在样本量较小或者数据偏斜时，可能不如其他方法准确。
- **基本法**：通过观察原始估计与重采样估计之间的偏差，调整置信区间。它考虑了偏差，但不像 BCa 法那样全面。
- **百分位法**：直接使用重采样分布的百分位数来计算置信区间。该方法简单直观，但可能忽略偏差校正。
- **BCa 法**：对偏差和加速效应进行校正，通常在样本量较小或数据偏斜时表现更好。

在样本量较小时，BCa 法通常是最稳健的，因为它对数据的偏差和加速效应进行了额外调整，而标准正态法则最简单，但在数据不正态时可能不可靠。



### 解释：

1. **数据部分**：提供故障时间的数据。
2. **最大似然估计**：根据题目 7.4，计算故障间隔时间 \( 1/\lambda \) 的最大似然估计。
3. **自助法置信区间**：使用 `boot` 包进行自助法抽样，并计算四种不同方法的 95% 置信区间。
4. **比较置信区间**：将四种方法的置信区间进行比较并展示。
5. **解释差异**：解释每种方法产生不同置信区间的原因。

## Answer 5

## 7.8 获取 \(\hat{\theta}\) 的偏差和标准误差的jackknife估计

### 数据准备
```{r}
library("bootstrap")
```
### R代码实现

#### Step 1: 计算 \(\hat{\theta}\)
```{R}

# 计算协方差矩阵
cov_matrix <- cov(scor)

# 计算协方差矩阵的特征值
eigenvalues <- eigen(cov_matrix)$values

# 计算 \hat{\theta} 的估计值
theta_hat <- eigenvalues[1] / sum(eigenvalues)
print(paste("Theta 的估计值:", theta_hat))
```

#### Step 2: 使用自助法估计 \(\hat{\theta}\) 的偏差和标准误差
```{R}
# 设置自助法重复次数
B <- 1000
theta_bootstrap <- numeric(B)

set.seed(123)  # 设置随机种子以确保可重复性

for (i in 1:B) {
  # 自助法抽样
  bootstrap_sample <- scor[sample(1:nrow(scor), replace = TRUE), ]
  
  # 计算自助样本的协方差矩阵
  bootstrap_cov <- cov(bootstrap_sample)
  
  # 计算特征值
  bootstrap_eigenvalues <- eigen(bootstrap_cov)$values
  
  # 计算 \hat{\theta} 值
  theta_bootstrap[i] <- bootstrap_eigenvalues[1] / sum(bootstrap_eigenvalues)
}

# 计算偏差和标准误差
bootstrap_bias <- mean(theta_bootstrap) - theta_hat
bootstrap_se <- sd(theta_bootstrap)

print(paste("自助法估计的偏差:", bootstrap_bias))
print(paste("自助法估计的标准误差:", bootstrap_se))
```

#### Step 3: 使用 Jackknife 方法估计 \(\hat{\theta}\) 的偏差和标准误差
```{R}
# 初始化Jackknife估计
n <- nrow(scor)
theta_jackknife <- numeric(n)

for (i in 1:n) {
  # 留出第 i 个样本
  jackknife_sample <- scor[-i, ]
  
  # 计算留一法样本的协方差矩阵
  jackknife_cov <- cov(jackknife_sample)
  
  # 计算特征值
  jackknife_eigenvalues <- eigen(jackknife_cov)$values
  
  # 计算 \hat{\theta} 值
  theta_jackknife[i] <- jackknife_eigenvalues[1] / sum(jackknife_eigenvalues)
}

# 计算偏差和标准误差
jackknife_bias <- (n - 1) * (mean(theta_jackknife) - theta_hat)
jackknife_se <- sqrt((n - 1) * mean((theta_jackknife - mean(theta_jackknife))^2))

print(paste("Jackknife法估计的偏差:", jackknife_bias))
print(paste("Jackknife法估计的标准误差:", jackknife_se))
```

## 7.10 在示例 7.18 中，使用了留一法（n 折）交叉验证来选择最佳拟合模型。重复此分析，将对数-对数模型替换为三次多项式模型。通过交叉验证程序选择了哪一个模型？根据最大调整 \( R^2 \) 选择了哪一个模型？
```{r}
# 加载必要的包
library(DAAG)

# 清除环境中的变量
rm(list = ls())

# 加载 ironslag 数据集
data("ironslag", package = "DAAG")
magnetic <- ironslag$magnetic
chemical <- ironslag$chemical

# 定义计算模型的预测误差的函数
cross_validation <- function(x, y) {
  n <- length(y)
  errors <- matrix(0, nrow = n, ncol = 4)
  
  for (k in 1:n) {
    # 留一法样本
    y_train <- y[-k]
    x_train <- x[-k]
    
    # 模型1：线性模型
    model1 <- lm(y_train ~ x_train)
    yhat1 <- predict(model1, newdata = data.frame(x_train = x[k]))
    errors[k, 1] <- (y[k] - yhat1)^2
    
    # 模型2：二次多项式模型
    model2 <- lm(y_train ~ x_train + I(x_train^2))
    yhat2 <- predict(model2, newdata = data.frame(x_train = x[k]))
    errors[k, 2] <- (y[k] - yhat2)^2
    
    # 模型3：三次多项式模型
    model3 <- lm(y_train ~ x_train + I(x_train^2) + I(x_train^3))
    yhat3 <- predict(model3, newdata = data.frame(x_train = x[k]))
    errors[k, 3] <- (y[k] - yhat3)^2
    
    # 模型4：对数-对数模型
    model4 <- lm(log(y_train) ~ log(x_train))
    logyhat4 <- predict(model4, newdata = data.frame(x_train = x[k]), type = "response")
    yhat4 <- exp(logyhat4)
    errors[k, 4] <- (y[k] - yhat4)^2
  }
  
  # 计算每个模型的均方误差
  mse <- colMeans(errors)
  names(mse) <- c("Linear", "Quadratic", "Cubic", "Log-Log")
  return(mse)
}

# 定义计算调整后 R^2 的函数
adjusted_r_squared <- function(x, y) {
  models <- list(
    lm(y ~ x),
    lm(y ~ x + I(x^2)),
    lm(y ~ x + I(x^2) + I(x^3)),
    lm(log(y) ~ log(x))
  )
  adj_r2 <- sapply(models, function(model) summary(model)$adj.r.squared)
  names(adj_r2) <- c("Linear", "Quadratic", "Cubic", "Log-Log")
  return(adj_r2)
}

# 运行交叉验证和计算调整后 R^2
mse_results <- cross_validation(chemical, magnetic)
adj_r2_results <- adjusted_r_squared(chemical, magnetic)

# 显示结果
print("均方误差 (MSE) 结果:")
print(mse_results)
print("调整后 R^2 结果:")
print(adj_r2_results)

# 清除内存
rm(list = ls())

```
1. **通过交叉验证（均方误差 MSE）选择最佳模型**：

   - **最佳模型**：均方误差最低的是二次多项式模型（MSE = 17.85248），因此根据交叉验证结果，二次多项式模型是最佳选择。

2. **根据最大调整后 \( R^2 \) 选择最佳模型**：

   - **最佳模型**：调整后 \( R^2 \) 最大的是二次多项式模型（\( R^2 = 0.5768151 \)），因此根据最大调整后 \( R^2 \) 的结果，二次多项式模型依然是最佳选择。

### 总结
无论是通过交叉验证的均方误差还是调整后的 \( R^2 \)，**二次多项式模型**都被选为最佳拟合模型。这表明二次多项式模型在该数据集上的拟合效果最优。


## 8.1 实现双样本 Cramér-von Mises 检验，以置换检验的形式来测试分布是否相等。将该检验应用于 R 中的 chickwts 数据集. 
```{r}
# 清除环境中的变量
rm(list = ls())

# 加载数据集
data("chickwts", package = "datasets")

# 提取豆粕和亚麻籽组的数据
x <- sort(as.vector(chickwts$weight[chickwts$feed == "soybean"]))
y <- sort(as.vector(chickwts$weight[chickwts$feed == "linseed"]))

# 定义Cramér-von Mises统计量计算函数
cvm_statistic <- function(x, y) {
  n <- length(x)
  m <- length(y)
  combined <- sort(c(x, y))
  Fx <- ecdf(x)
  Fy <- ecdf(y)
  statistic <- sum((Fx(combined) - Fy(combined))^2) * (n * m) / (n + m)
  return(statistic)
}

# 置换检验函数
cvm_permutation_test <- function(x, y, num_permutations = 1000) {
  observed_stat <- cvm_statistic(x, y)
  combined <- c(x, y)
  n <- length(x)
  
  perm_stats <- numeric(num_permutations)
  for (i in 1:num_permutations) {
    permuted <- sample(combined)
    x_perm <- permuted[1:n]
    y_perm <- permuted[(n+1):length(permuted)]
    perm_stats[i] <- cvm_statistic(x_perm, y_perm)
  }
  
  # 计算p值
  p_value <- mean(perm_stats >= observed_stat)
  
  list(statistic = observed_stat, p_value = p_value)
}

# 执行置换检验
set.seed(123)  # 设置随机种子以保证可重复性
result <- cvm_permutation_test(x, y, num_permutations = 1000)

# 打印结果
print(paste("Cramér-von Mises 统计量:", result$statistic))
print(paste("p 值:", result$p_value))

# 清除内存
rm(list = ls())

```

1. **Cramér-von Mises 统计量**：
   - 计算得到的 Cramér-von Mises 统计量值为 3.93956043956044。这个值表示豆粕组和亚麻籽组的经验分布函数之间的差异大小。

2. **p 值**：
   - 置换检验的 p 值为 0.422。这个 p 值表示在 0.422 的概率下，我们可能会观察到等于或大于观测到的统计量差异。
   - 通常在显著性水平（例如 0.05）下，如果 p 值大于 0.05，我们无法拒绝原假设。因此，p 值为 0.422 远大于 0.05，说明我们无法拒绝豆粕组和亚麻籽组的分布相等的原假设。

### 总结
- **结论**：基于 Cramér-von Mises 检验的结果，我们没有足够的证据拒绝豆粕组和亚麻籽组在重量分布上的相等性假设。这意味着在统计上，这两个组的分布没有显著差异。



## 8.2 实现双变量Spearman秩相关性检验来检验独立性，以置换检验的形式。Spearman秩相关性检验的统计量可以通过函数 cor 并设置 method = "spearman" 获得。将置换检验的显著性水平与 cor.test 在相同样本上报告的 p 值进行比较。
```{r}
# 清除环境中的变量
rm(list = ls())

# 定义计算Spearman秩相关性的函数
spearman_correlation <- function(x, y) {
  return(cor(x, y, method = "spearman"))
}

# 定义置换检验函数
spearman_permutation_test <- function(x, y, num_permutations = 1000) {
  # 计算观察到的Spearman相关性
  observed_stat <- spearman_correlation(x, y)
  
  # 初始化置换统计量
  perm_stats <- numeric(num_permutations)
  
  # 进行置换
  for (i in 1:num_permutations) {
    y_permuted <- sample(y)  # 随机置换y
    perm_stats[i] <- spearman_correlation(x, y_permuted)
  }
  
  # 计算p值
  p_value <- mean(abs(perm_stats) >= abs(observed_stat))
  
  # 返回结果
  list(statistic = observed_stat, p_value = p_value)
}

# 测试数据
set.seed(123)  # 设置随机种子保证结果可重复
x <- rnorm(30)  # 随机生成数据
y <- x + rnorm(30) * 0.5  # 添加噪声的相关数据

# 执行置换检验
result_permutation <- spearman_permutation_test(x, y, num_permutations = 1000)

# 使用 cor.test 计算 p 值
result_cor_test <- cor.test(x, y, method = "spearman")

# 显示结果
print(paste("置换检验的Spearman相关性统计量:", result_permutation$statistic))
print(paste("置换检验的p值:", result_permutation$p_value))
print(paste("cor.test的p值:", result_cor_test$p.value))

# 清除内存
rm(list = ls())

```

1. **置换检验的Spearman相关性统计量**：
   - 计算得到的Spearman相关性统计量为 \(0.90567296996663\)。这个值表示变量 \(x\) 和 \(y\) 之间的强正相关性。

2. **置换检验的p值**：
   - 置换检验的p值为 0，这表明在1000次置换中，没有一次置换导致的相关性统计量等于或超过了观察到的相关性值。这说明变量 \(x\) 和 \(y\) 的相关性在统计上非常显著。

3. **cor.test 的 p 值**：
   - 使用 `cor.test` 计算的 p 值为 \(2.17768693310597 \times 10^{-7}\)，也非常接近零。这表明变量 \(x\) 和 \(y\) 之间的相关性在显著性水平（例如0.05）下非常显著。

### 总结
- **结论**：无论是通过置换检验还是 `cor.test`，我们都可以得出变量 \(x\) 和 \(y\) 之间存在显著正相关的结论。两种方法的p值都接近零，进一步验证了这种显著性。

## Answer 6

## 9.3
使用Metropolis-Hastings采样器从标准柯西分布中生成随机变量。舍弃链的前1000个样本，并将生成的观测值的十分位数与标准柯西分布的十分位数进行比较（请参见`qcauchy`或`qt`，设置自由度df=1）。回顾一下，柯西分布\(Cauchy(\theta, \eta)\)的密度函数为：

\[
f(x) = \frac{1}{\theta \pi (1 + \left(\frac{x - \eta}{\theta}\right)^2)}, \quad -\infty < x < \infty, \quad \theta > 0。
\]

标准柯西分布的参数为\(\theta = 1\)和\(\eta = 0\)。（注意，标准柯西密度等价于自由度为1的学生t分布密度。）
增加使用Gelman-Rubin方法监控链的收敛性，并运行链直到它根据 \(\hat{R} < 1.2\) 大致收敛到目标分布。
```{r}
# 设置参数
theta <- 1  # 标准柯西分布的尺度参数
eta <- 0    # 标准柯西分布的位置参数
n <- 10000  # 每条链生成的样本数量
burn_in <- 1000  # 舍弃的样本数
chains <- 4  # 生成的链数

# 1. 定义目标密度函数 (标准柯西分布的概率密度函数)
dcauchy_density <- function(x, theta, eta) {
  return(1 / (theta * pi * (1 + ((x - eta) / theta)^2)))
}

# 2. Metropolis-Hastings采样函数
metropolis_hastings <- function(n, burn_in, theta, eta) {
  samples <- numeric(n)  # 预分配内存
  samples[1] <- 0  # 初始值
  
  for (i in 2:n) {
    # 提议分布: 使用正态分布提议
    proposal <- rnorm(1, mean = samples[i-1], sd = 1)
    
    # 计算接受率
    acceptance_ratio <- dcauchy_density(proposal, theta, eta) / dcauchy_density(samples[i-1], theta, eta)
    
    # 接受或拒绝
    if (runif(1) < acceptance_ratio) {
      samples[i] <- proposal
    } else {
      samples[i] <- samples[i-1]
    }
  }
  
  # 舍弃前 burn_in 个样本
  return(samples[(burn_in + 1):n])
}

# 3. 计算Gelman-Rubin统计量
gelman_rubin <- function(chains) {
  m <- length(chains)
  # 计算各链均值和方差
  means <- sapply(chains, mean)
  variances <- sapply(chains, var)
  
  # 计算整体均值和整体方差
  overall_mean <- mean(means)
  B <- sum((means - overall_mean)^2) * n / (m - 1)  # between-chain variance
  W <- mean(variances)  # within-chain variance
  var_hat <- (n - 1) / n * W + B / n  # combined variance
  
  return(sqrt(var_hat / W))  # Gelman-Rubin统计量
}

# 4. 运行链直到收敛
run_chains_until_convergence <- function(n, burn_in, theta, eta, chains) {
  chain_samples <- lapply(1:chains, function(x) metropolis_hastings(n, burn_in, theta, eta))
  hat_R <- gelman_rubin(chain_samples)
  
  while (hat_R >= 1.2) {
    cat("当前 \\hat{R}: ", hat_R, "\n")
    chain_samples <- lapply(1:chains, function(x) metropolis_hastings(n, burn_in, theta, eta))
    hat_R <- gelman_rubin(chain_samples)
  }
  
  cat("最终 \\hat{R}: ", hat_R, "\n")  # 输出最终的Gelman-Rubin统计量
  return(chain_samples)
}

# 5. 计算并比较十分位数的函数
compare_quantiles <- function(samples, theta, eta) {
  sample_quantiles <- quantile(samples, probs = seq(0.1, 0.9, by = 0.1))
  theoretical_quantiles <- qcauchy(seq(0.1, 0.9, by = 0.1), location = eta, scale = theta)
  
  cat("生成样本的十分位数：\n")
  print(sample_quantiles)
  
  cat("\n标准柯西分布的理论十分位数：\n")
  print(theoretical_quantiles)
}

# 6. 可视化函数
plot_samples <- function(samples, theta, eta) {
  hist(samples, breaks = 50, probability = TRUE, main = "Metropolis-Hastings 生成的标准柯西分布样本",
       xlab = "样本值", ylab = "密度")
  curve(dcauchy(x, location = eta, scale = theta), col = "red", lwd = 2, add = TRUE)
  legend("topright", legend = c("生成样本密度", "理论密度"), col = c("black", "red"), lwd = 2)
}

# 主程序
set.seed(42)  # 设置随机种子
chain_samples <- run_chains_until_convergence(n, burn_in, theta, eta, chains)  # 运行链直到收敛

# 计算并输出每条链的十分位数比较结果
for (i in 1:chains) {
  cat(paste("链", i, "的十分位数比较：\n"))
  compare_quantiles(chain_samples[[i]], theta, eta)
  plot_samples(chain_samples[[i]], theta, eta)
}

# 清理内存
rm(chain_samples)
gc()
```

## 9.8
9.8 该示例出现在文献[40]中。考虑二元密度

\[
f(x, y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1}, \quad x = 0, 1, \dots, n, \quad 0 \leq y \leq 1。
\]

可以证明（参见例如文献[23]），对于固定的 \(a\)、\(b\)、\(n\)，条件分布为二项分布\(Binomial(n, y)\)和贝塔分布\(Beta(x + a, n - x + b)\)。使用Gibbs采样器生成以 \(f(x, y)\) 为目标联合密度的链。
增加使用Gelman-Rubin方法监控链的收敛性，并运行链直到它根据 \(\hat{R} < 1.2\) 大致收敛到目标分布。
```{r}
# 加载必要的包
library(coda)
library(ggplot2)

# 设置参数
a <- 2  # a的值
b <- 2  # b的值
n <- 10 # n的值
num_samples <- 20000  # 增加采样数量
burn_in <- 2000       # 适当增加burn-in
chains <- 4           # 生成的链数

# 1. 定义从条件分布Binomial(n, y)采样的函数
sample_x_given_y <- function(y, n) {
  return(rbinom(1, size = n, prob = y))
}

# 2. 定义从条件分布Beta(x + a, n - x + b)采样的函数
sample_y_given_x <- function(x, a, b, n) {
  return(rbeta(1, shape1 = x + a, shape2 = n - x + b))
}

# 3. Gibbs采样函数，随机初始化每条链的x和y
gibbs_sampler <- function(num_samples, burn_in, a, b, n) {
  samples <- matrix(NA, nrow = num_samples, ncol = 2)
  x <- sample(0:n, 1)           # 随机选择初始x
  y <- runif(1, 0, 1)           # 随机选择初始y
  
  for (i in 1:num_samples) {
    x <- sample_x_given_y(y, n)
    y <- sample_y_given_x(x, a, b, n)
    samples[i, ] <- c(x, y)
  }
  
  return(samples[(burn_in + 1):num_samples, ])
}

# 4. 计算Gelman-Rubin统计量
run_chains_until_convergence <- function(num_samples, burn_in, a, b, n, chains) {
  chain_samples <- lapply(1:chains, function(x) gibbs_sampler(num_samples, burn_in, a, b, n))
  chain_samples_mcmc <- lapply(chain_samples, function(samples) as.mcmc(samples))
  
  gelman_diag_result <- gelman.diag(mcmc.list(chain_samples_mcmc))
  hat_R <- gelman_diag_result$psrf
  
  while (any(hat_R >= 1.2)) {
    cat("当前 \\hat{R}: ", hat_R, "\n")
    chain_samples <- lapply(1:chains, function(x) gibbs_sampler(num_samples, burn_in, a, b, n))
    chain_samples_mcmc <- lapply(chain_samples, function(samples) as.mcmc(samples))
    gelman_diag_result <- gelman.diag(mcmc.list(chain_samples_mcmc))
    hat_R <- gelman_diag_result$psrf
  }
  
  cat("最终 \\hat{R}: ", hat_R, "\n")
  return(chain_samples)
}

# 5. 定义联合密度函数
joint_density <- function(x, y, a, b, n) {
  coeff <- choose(n, x)  # 二项式系数
  density <- coeff * (y^(x + a - 1)) * ((1 - y)^(n - x + b - 1))
  return(density)
}

# 6. 可视化函数：绘制x和y的联合分布图
plot_samples <- function(samples) {
  plot(samples[, 1], samples[, 2], pch = 20, col = "blue",
       xlab = "x", ylab = "y", main = "Gibbs采样生成的(x, y)联合分布")
}

# 7. 可视化目标联合密度
plot_joint_density <- function(a, b, n) {
  x_values <- 0:n
  y_values <- seq(0, 1, length.out = 100)
  
  # 创建数据框以存储密度值
  density_data <- expand.grid(x = x_values, y = y_values)
  density_data$f_xy <- mapply(joint_density, density_data$x, density_data$y, MoreArgs = list(a = a, b = b, n = n))
  
  # 绘制密度图
  ggplot(density_data, aes(x = y, y = x, fill = f_xy)) +
    geom_raster(interpolate = TRUE) +
    scale_fill_viridis_c() +
    labs(title = "Joint Density f(x, y)",
         x = "y",
         y = "x",
         fill = "Density") +
    theme_minimal()
}

# 主程序
set.seed(42)  # 设置随机种子
chain_samples <- run_chains_until_convergence(num_samples, burn_in, a, b, n, chains)  # 运行链直到收敛

# 可视化生成的每条链的样本
for (i in 1:chains) {
  cat(paste("链", i, "的样本可视化：\n"))
  plot_samples(chain_samples[[i]])
}

# 可视化目标联合密度
plot_joint_density(a, b, n)

# 清理内存
rm(chain_samples, a, b, n, num_samples, burn_in, chains)
gc()
```

## 连续情况下的 Metropolis-Hastings 采样器的平稳性证明

要证明连续情况下的 Metropolis-Hastings (MH) 采样器的平稳性，我们需要验证目标分布 $\pi(x)$ 是马尔科夫链的平稳分布。即，若当前状态的分布为 $\pi(x)$，则下一个状态的分布也应为 $\pi(x)$。

我们用 $p(x'|x)$ 表示 MH 采样器的转移核，定义为从状态 $x$ 转移到状态 $x'$ 的概率。平稳性条件可以表示为：

$$
\int \pi(x) p(x'|x) dx = \pi(x')
$$

MH 采样器的转移核由提议分布 $q(x'|x)$ 和接受概率 $A(x'|x)$ 组成：

$$
p(x'|x) = q(x'|x) A(x'|x) + \delta(x'-x) \left( 1 - \int q(x''|x) A(x''|x) dx'' \right)
$$

其中，$\delta(x'-x)$ 是狄拉克δ函数，表示如果提议被拒绝，则保持当前状态。接受概率 $A(x'|x)$ 定义为：

$$
A(x'|x) = \min\left(1, \frac{\pi(x')q(x|x')}{\pi(x)q(x'|x)}\right)
$$

为了证明平稳性，我们只需证明 $\pi(x) p(x'|x)$ 满足细致平衡条件：

$$
\pi(x) p(x'|x) = \pi(x') p(x|x')
$$

如果细致平衡条件成立，则平稳性条件也成立。

### 细致平衡条件证明

1. **情况一：** 如果 $\pi(x')q(x|x') \ge \pi(x)q(x'|x)$，则 $A(x'|x) = 1$，$A(x|x') = \frac{\pi(x)q(x'|x)}{\pi(x')q(x|x')}$。因此：

   $$
   \pi(x) p(x'|x) = \pi(x) q(x'|x)
   $$

   $$ 
   \pi(x') p(x|x') = \pi(x') q(x|x') A(x|x') = \pi(x') q(x|x') \frac{\pi(x)q(x'|x)}{\pi(x')q(x|x')} = \pi(x) q(x'|x)
   $$

   所以，$\pi(x) p(x'|x) = \pi(x') p(x|x')$。

2. **情况二：** 如果 $\pi(x')q(x|x') < \pi(x)q(x'|x)$，则 $A(x'|x) = \frac{\pi(x')q(x|x')}{\pi(x)q(x'|x)}$，$A(x|x') = 1$。因此：

   $$
   \pi(x) p(x'|x) = \pi(x) q(x'|x) \frac{\pi(x')q(x|x')}{\pi(x)q(x'|x)} = \pi(x') q(x|x')
   $$

   $$
   \pi(x') p(x|x') = \pi(x') q(x|x')
   $$

   所以，$\pi(x) p(x'|x) = \pi(x') p(x|x')$。

### 结论

在两种情况下，细致平衡条件都成立。因此，目标分布 $\pi(x)$ 是 MH 采样器的平稳分布，这证明了 MH 采样器的平稳性。综上，我们证明了连续情况下的 Metropolis-Hastings 采样器的平稳性。这意味着如果马尔可夫链的当前状态服从目标分布 $\pi(x)$，那么下一个状态也将服从 $\pi(x)$，保证了采样算法的正确性。

## Answer 7
## 11.3
(a) 编写一个函数来计算以下和式中的第 \( k \) 项：
\[
\sum_{k=0}^{\infty} \frac{(-1)^k}{k! \, 2^k} \frac{\|a\|^{2k+2}}{(2k+1)(2k+2)} \frac{\Gamma\left(\frac{d+1}{2}\right) \Gamma\left(k + \frac{3}{2}\right)}{\Gamma\left(k + \frac{d}{2} + 1\right)},
\]
其中 \( d \geq 1 \) 是一个整数， \( a \) 是 \( \mathbb{R}^d \) 中的一个向量，且 \( \|\cdot\| \) 表示欧几里得范数。执行运算时确保系数能够被计算出来，即使 \( k \) 和 \( d \) 的值相当大。（对于所有 \( a \in \mathbb{R}^d \)，该级数都会收敛。）
(b) 修改该函数，使其计算并返回整个和的值。
(c) 当 \( a = (1, 2)^T \) 时，求和的值。
```{r}
# 清空环境变量，释放内存
rm(list = ls())
gc()

# 加载必要的包
if(!requireNamespace("pracma", quietly = TRUE)) install.packages("pracma")
if(!requireNamespace("gsl", quietly = TRUE)) install.packages("gsl")
library(pracma)  # 用于计算伽马函数
library(gsl)     # 提供更高效的伽马函数计算

# 定义一个函数来计算第 k 项
kth_term <- function(k, a_norm, d) {
  # 计算分子的 (-1)^k / (k! * 2^k)
  term1 <- (-1)^k / (factorial(k) * 2^k)
  
  # 计算分子的 ||a||^(2k + 2) / ((2k + 1)(2k + 2))
  term2 <- a_norm^(2 * k + 2) / ((2 * k + 1) * (2 * k + 2))
  
  # 计算伽马函数项
  gamma_term <- gamma((d + 1) / 2) * gamma(k + 3 / 2) / gamma(k + d / 2 + 1)
  
  # 计算第 k 项的值
  result <- term1 * term2 * gamma_term
  return(result)
}

# 定义计算整个和的函数
compute_sum <- function(a, d, tolerance = 1e-10, max_iter = 1000) {
  # 计算向量 a 的欧几里得范数 ||a||
  a_norm <- norm(a, type = "2")
  
  # 初始化总和
  total_sum <- 0
  k <- 0
  
  # 计算逐项累加，直到项小于容忍度或达到最大迭代次数
  repeat {
    term <- kth_term(k, a_norm, d)
    total_sum <- total_sum + term
    
    # 如果当前项小于容忍度，则停止
    if (abs(term) < tolerance) break
    # 如果达到最大迭代次数，也停止
    if (k >= max_iter) {
      warning("达到最大迭代次数，结果可能不准确")
      break
    }
    
    # 更新迭代计数器
    k <- k + 1
  }
  
  return(total_sum)
}

# 测试程序
# 清空内存
gc()

# (c) 当 a = (1, 2)^T 时计算和的值
a <- c(1, 2)
d <- length(a)
result <- compute_sum(a, d)

# 输出结果
cat("当 a = (1, 2)^T 时的和为:", result, "\n")

```
# 11.5
编写一个函数以求解如下方程：
\[
\frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)} \Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}} \left(1 + \frac{u^2}{k-1}\right)^{-\frac{k}{2}} \, du = \frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k} \Gamma\left(\frac{k}{2}\right)} \int_{0}^{c_k} \left(1 + \frac{u^2}{k}\right)^{-\frac{k+1}{2}} \, du
\]
对于参数 \( a \)，其中
\[
c_k = \sqrt{\frac{a^2 k}{k + 1 - a^2}}
\]

将求得的解与习题 11.4 中的点 \( A(k) \) 进行比较。

```{r}
# 清空环境变量，释放内存
rm(list = ls())
gc()

# 加载必要的包
if (!requireNamespace("pracma", quietly = TRUE)) install.packages("pracma")
library(pracma)     # 用于计算伽马函数

# 定义计算积分的函数
integrate_func <- function(f, lower, upper, k) {
  tryCatch({
    # 使用 base R 的 integrate 函数以更好地控制积分误差
    result <- integrate(function(u) f(u, k), lower, upper)$value
    return(result)
  }, error = function(e) {
    # 如果积分失败，返回一个大值表示错误
    return(1e10)
  })
}

# 定义方程中的积分核函数
integrand_left <- function(u, k) {
  (1 + u^2 / (k - 1))^(-k / 2)
}

integrand_right <- function(u, k) {
  (1 + u^2 / k)^(-(k + 1) / 2)
}

# 定义函数来求解方程
solve_for_a <- function(k, tolerance = 1e-6, max_iter = 100) {
  # 伽马函数项
  gamma_left <- 2 * gamma(k / 2) / (sqrt(pi * (k - 1)) * gamma((k - 1) / 2))
  gamma_right <- 2 * gamma((k + 1) / 2) / (sqrt(pi * k) * gamma(k / 2))
  
  # 定义误差函数，用于优化求解 a
  error_func <- function(a) {
    # 确保 a 的范围有效，避免出现分母为零的情况
    if (a^2 >= k || a^2 >= k + 1) return(1e10)
    
    # 计算 c_k 和 c_{k-1}，并检查是否接近无效的情况
    denominator_k_minus1 <- k - a^2
    denominator_k <- k + 1 - a^2
    if (denominator_k_minus1 <= 0 || denominator_k <= 0) {
      return(1e10)  # 返回大值表示错误
    }
    
    c_k_minus1 <- sqrt(a^2 * (k - 1) / denominator_k_minus1)
    c_k <- sqrt(a^2 * k / denominator_k)
    
    # 动态设置积分上限
    max_limit <- min(sqrt(k) * 10, 1e3)
    if (c_k_minus1 > max_limit || c_k > max_limit) {
      return(1e10)  # 返回大值以跳过这个计算
    }
    
    # 计算左侧积分和右侧积分
    integral_left <- integrate_func(integrand_left, 0, c_k_minus1, k)
    integral_right <- integrate_func(integrand_right, 0, c_k, k)
    
    # 检查是否为有效数值，主动将 NA/Inf 替换为大值
    if (!is.finite(integral_left) || !is.finite(integral_right) || is.nan(integral_left) || is.nan(integral_right)) {
      return(1e10)  # 主动返回一个大值
    }
    
    # 计算误差
    error_value <- abs(gamma_left * integral_left - gamma_right * integral_right)
    return(error_value)
  }
  
  # 使用 suppressWarnings 包裹 optimize 以隐藏警告
  result <- suppressWarnings(optimize(error_func, interval = c(0.2, sqrt(k) - 1), tol = tolerance))
  return(result$minimum)
}

# 计算 A(k) 的点，进行比较
compute_A_k <- function(k) {
  return(sqrt(k))
}

# 主函数，运行求解并比较结果
main <- function() {
  # 设置参数 k 的值
  k_values <- c(4, 25, 100, 500, 1000)
  results <- data.frame(k = integer(), a = numeric(), A_k = numeric())
  
  for (k in k_values) {
    # 求解 a 的值
    a <- solve_for_a(k)
    
    # 计算 A(k)
    A_k <- compute_A_k(k)
    
    # 存储结果
    results <- rbind(results, data.frame(k = k, a = a, A_k = A_k))
  }
  
  # 输出结果
  print(results)
  
  # 清空内存
  gc()
}

# 运行主函数
main()


```


- 假设 \( T_1, \ldots, T_n \) 是从具有期望值为 \( \lambda \) 的指数分布中抽取的独立同分布样本。由于右删失，超过阈值 \( \tau \) 的值无法观察到，因此观察到的值为
  \[
  Y_i = T_i \mathbb{I}(T_i \leq \tau) + \tau \mathbb{I}(T_i > \tau), \quad i = 1, \ldots, n。
  \]
  假设 \( \tau = 1 \)，并且观察到的 \( Y_i \) 值如下：

  \[
  0.54, 0.48, 0.33, 0.43, 1.00, 0.91, 1.00, 0.21, 0.85
  \]

- 使用 EM 算法来估计 \( \lambda \)，并将结果与观测数据的最大似然估计（MLE）进行比较（注意：\( Y_i \) 遵循一个混合分布）。

```{r}
# 清理内存并设置环境
rm(list = ls()) # 清空所有内存
gc()            # 垃圾回收

# 检查和安装所需的包
if (!require("stats")) install.packages("stats")

# 定义 EM 算法的函数
em_algorithm <- function(observed_data, tau, max_iter = 1000, tol = 1e-6) {
  n <- length(observed_data)
  # 初始估计的 lambda
  lambda <- 1 / mean(observed_data) 
  
  # EM 迭代
  for (iter in 1:max_iter) {
    # E 步：计算每个观测值的 "未观测部分" 的期望
    expected_values <- ifelse(observed_data < tau, observed_data, 
                              tau + 1 / lambda)
    
    # M 步：使用期望值更新 lambda
    new_lambda <- n / sum(expected_values)
    
    # 检查收敛
    if (abs(new_lambda - lambda) < tol) {
      lambda <- new_lambda
      break
    }
    
    # 更新 lambda
    lambda <- new_lambda
  }
  
  return(lambda)
}

# 定义最大似然估计（MLE）的函数
mle_estimate <- function(observed_data) {
  return(1 / mean(observed_data)) # MLE for lambda based on observed data
}

# 观测数据和参数
observed_data <- c(0.54, 0.48, 0.33, 0.43, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1

# 使用 EM 算法估计 lambda
lambda_em <- em_algorithm(observed_data, tau)
cat("EM 算法估计的 lambda 值：", lambda_em, "\n")

# 计算 MLE 估计
lambda_mle <- mle_estimate(observed_data)
cat("基于观测数据的 MLE 估计的 lambda 值：", lambda_mle, "\n")

# 再次清空内存以确保资源释放
rm(list = ls())
gc() # 垃圾回收

```

## Answer 8
## 11.7
Use the simplex algorithm to solve the following problem.

 \[\text{min}\ 4x + 2y + 9z \]
 
 subject to:
\[
2x + y + z \leq 2
\]
\[
x - y + 3z \leq 3
\]
\[
x \geq 0, \, y \geq 0, \, z \geq 0
\]

```{r}
# 加载 lpSolve 包
library(lpSolve)

# 定义目标函数的系数
objective <- c(4, 2, 9)  # 对应于 4x + 2y + 9z

# 定义约束矩阵
constraints <- matrix(c(2, 1, 1,   # 2x + y + z
                        1, -1, 3), # x - y + 3z
                      nrow = 2, byrow = TRUE)

# 定义约束右侧的值
rhs <- c(2, 3)  # 右侧的约束值

# 定义约束方向
directions <- c("<=", "<=")

# 使用 lp 函数求解最小化问题
solution <- lp(direction = "min",      # 指定为最小化问题
               objective.in = objective, # 目标函数
               const.mat = constraints,  # 约束矩阵
               const.dir = directions,   # 约束方向
               const.rhs = rhs,          # 约束右侧值
               all.int = FALSE)          # 变量可以是连续的

# 输出结果
cat("目标函数的最小值：", solution$objval, "\n")
cat("对应的变量值：\n")
cat("x =", solution$solution[1], "\n")
cat("y =", solution$solution[2], "\n")
cat("z =", solution$solution[3], "\n")
```
## 3
使用 for 循环和 lapply() 对 mtcars 数据集进行线性模型拟合，使用存储在以下列表中的公式：

formulas <- list(  
    mpg ~ disp,  
    mpg ~ I(1 / disp),  
    mpg ~ disp + wt,  
    mpg ~ I(1 / disp) + wt  
)


```{r}
# 加载 mtcars 数据集
data(mtcars)

# 定义公式列表
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

# 方法一：使用 for 循环拟合模型
models_for <- list()  # 用于存储模型结果
for (i in seq_along(formulas)) {
  models_for[[i]] <- lm(formulas[[i]], data = mtcars)
}

# 打印 for 循环拟合的模型结果摘要
cat("For 循环拟合结果摘要：\n")
for (i in seq_along(models_for)) {
  print(summary(models_for[[i]]))
}

# 方法二：使用 lapply() 拟合模型
models_lapply <- lapply(formulas, function(f) lm(f, data = mtcars))

# 打印 lapply() 拟合的模型结果摘要
cat("\nlapply() 拟合结果摘要：\n")
lapply(models_lapply, summary)

```
# 4


4. 将模型 **mpg ~ disp** 拟合到下面列表中的 **mtcars 数据集的自助法重采样（bootstrap replicates）** 上，分别使用 **for 循环** 和 **lapply()** 完成。  
你能在不用匿名函数的情况下完成吗？

bootstraps <- lapply(1:10, function(i) {  
  rows <- sample(1:nrow(mtcars), rep = TRUE)  
  mtcars[rows, ]  
})


```{r}
# 清空环境变量以节约内存
rm(list = ls())

# 加载数据集
data(mtcars)

# 定义函数：生成 Bootstrap 数据集
generate_bootstrap <- function(data, n) {
  # 创建 n 个 bootstrap 样本
  lapply(1:n, function(i) {
    rows <- sample(1:nrow(data), size = nrow(data), replace = TRUE)
    data[rows, ]
  })
}

# 定义函数：拟合线性模型
fit_model <- function(data_list, formula) {
  # 针对每个数据子集拟合模型
  lapply(data_list, function(sub_data) lm(formula, data = sub_data))
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 步骤 1：生成 10 个 Bootstrap 样本
bootstraps <- generate_bootstrap(mtcars, 10)

# 步骤 2：拟合模型 mpg ~ disp 到每个 Bootstrap 样本
models <- fit_model(bootstraps, mpg ~ disp)

# 步骤 3：检查模型结果（简要输出系数）
cat("模型回归系数:\n")
lapply(models, function(model) print(coef(model)))

# 清空内存以节省空间
clean_memory()
```

## 5
对于前两个练习中的每个模型，使用下面的函数提取 **R²** 值。

```R
rsq <- function(mod) summary(mod)$r.squared
```
### 对任务3
```{r}
# 清空环境变量
rm(list = ls())

# 加载数据集
data(mtcars)

# 定义公式列表
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

# 定义函数：拟合模型（for 循环实现）
fit_models_for <- function(formulas, data) {
  models <- list()  # 存储模型
  for (i in seq_along(formulas)) {
    models[[i]] <- lm(formulas[[i]], data = data)
  }
  models
}

# 定义函数：拟合模型（lapply 实现）
fit_models_lapply <- function(formulas, data) {
  lapply(formulas, function(f) lm(f, data = data))
}

# 定义函数：提取 R² 值
extract_r_squared <- function(models) {
  # 使用 lapply 提取每个模型的 R² 值
  sapply(models, function(model) summary(model)$r.squared)
}

# 步骤1：使用 for 循环拟合模型
models_for <- fit_models_for(formulas, mtcars)

# 步骤2：使用 lapply 拟合模型
models_lapply <- fit_models_lapply(formulas, mtcars)

# 步骤3：提取 R² 值（for 循环模型）
r_squared_for <- extract_r_squared(models_for)

# 步骤4：提取 R² 值（lapply 模型）
r_squared_lapply <- extract_r_squared(models_lapply)

# 输出 R² 值
cat("For 循环模型的 R² 值：\n")
print(r_squared_for)

cat("\nlapply 模型的 R² 值：\n")
print(r_squared_lapply)

# 清理内存
rm(list = ls())
gc()

```
### 对任务4
```{r}
# 清空环境变量以节约内存
rm(list = ls())

# 加载数据集
data(mtcars)

# 定义函数：生成 Bootstrap 数据集
generate_bootstrap <- function(data, n) {
  # 创建 n 个 bootstrap 样本
  lapply(1:n, function(i) {
    rows <- sample(1:nrow(data), size = nrow(data), replace = TRUE)
    data[rows, ]
  })
}

# 定义函数：拟合线性模型
fit_model <- function(data_list, formula) {
  # 针对每个数据子集拟合模型
  lapply(data_list, function(sub_data) lm(formula, data = sub_data))
}

# 定义函数：提取 R² 值
extract_r_squared <- function(models) {
  # 提取每个模型的 R² 值
  sapply(models, function(model) summary(model)$r.squared)
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 步骤 1：生成 10 个 Bootstrap 样本
bootstraps <- generate_bootstrap(mtcars, 10)

# 步骤 2：拟合模型 mpg ~ disp 到每个 Bootstrap 样本
models <- fit_model(bootstraps, mpg ~ disp)

# 步骤 3：提取模型的 R² 值
r_squared_values <- extract_r_squared(models)

# 打印 R² 值
cat("每个模型的 R² 值：\n")
print(r_squared_values)

# 清空内存以节省空间
clean_memory()

```

## 3(P213)
以下代码模拟了非正态分布数据下 t 检验的性能。使用 **sapply()** 和匿名函数提取每次试验的 **p 值**。

```R
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```

额外挑战：通过直接使用 **[[** 消除匿名函数。

```{r}
# 清空环境变量
rm(list = ls())

# 定义函数：生成 t 检验结果
generate_t_tests <- function(n) {
  replicate(
    n,
    t.test(rpois(10, 10), rpois(7, 10)),
    simplify = FALSE
  )
}

# 定义函数：提取 p 值
extract_p_values <- function(t_tests) {
  # 使用 sapply 提取 p 值，直接使用 [[ 消除匿名函数
  sapply(t_tests, "[[", "p.value")
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 步骤1：生成 100 次 t 检验
t_test_results <- generate_t_tests(100)

# 步骤2：提取每次试验的 p 值
p_values <- extract_p_values(t_test_results)

# 打印 p 值
cat("提取的 p 值：\n")
print(p_values)

# 清空内存
clean_memory()
```

## 6(P214)
实现一个结合 **Map()** 和 **vapply()** 的功能，创建一个变体版本的 **lapply()**，能够对所有输入进行并行迭代，并将输出存储为向量（或矩阵）。  
这个函数应该接受哪些参数？
```R
custom_lapply_variant <- function(FUN, ..., output_type = "vector", USE.NAMES = TRUE) {
  # 参数说明：
  # FUN: 用户自定义的操作函数
  # ...: 不定数量的并行输入对象（列表、向量等）
  # output_type: 输出类型（"vector" 或 "matrix"）
  # USE.NAMES: 是否保留输出的名称（默认 TRUE）
}
```

```{r}
# 清空环境变量
rm(list = ls())

# 定义函数：结合 Map 和 vapply 实现并行迭代，并存储为向量或矩阵
custom_lapply_variant <- function(FUN, ..., output_type = "vector") {
  # 检查 output_type 参数是否有效
  if (!output_type %in% c("vector", "matrix")) {
    stop("Invalid output_type. Choose 'vector' or 'matrix'.")
  }
  
  # 使用 Map 进行并行迭代
  map_results <- Map(FUN, ...)
  
  # 根据 output_type 转换结果
  if (output_type == "vector") {
    # 转换为向量
    return(unlist(map_results))
  } else if (output_type == "matrix") {
    # 转换为矩阵
    return(do.call(rbind, map_results))
  }
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 示例1：对两个向量的对应元素求和，并存储为向量
vector_result <- custom_lapply_variant(
  FUN = `+`, 
  1:5, 
  6:10, 
  output_type = "vector"
)

cat("结果存储为向量：\n")
print(vector_result)

# 示例2：对两个向量的对应元素求和，并存储为矩阵
matrix_result <- custom_lapply_variant(
  FUN = function(x, y) c(sum = x + y, product = x * y), 
  1:5, 
  6:10, 
  output_type = "matrix"
)

cat("\n结果存储为矩阵：\n")
print(matrix_result)

# 清空内存
clean_memory()
```

## 4(P365)
制作一个更快版本的 `chisq.test()`，仅在输入为两个没有缺失值的数值向量时计算卡方检验的统计量。  
你可以通过简化 `chisq.test()` 或根据数学定义直接编写代码实现（参考 [Pearson's chi-squared test](http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)）。

```{r}
# 清空环境变量
rm(list = ls())

# 定义函数：快速计算卡方检验统计量
fast_chisq_test <- function(x, y) {
  # 检查输入是否为数值向量
  if (!is.numeric(x) || !is.numeric(y)) {
    stop("Inputs must be numeric vectors.")
  }
  
  # 检查输入是否有缺失值
  if (any(is.na(x)) || any(is.na(y))) {
    stop("Inputs must not contain missing values.")
  }
  
  # 检查输入长度是否匹配
  if (length(x) != length(y)) {
    stop("Inputs must have the same length.")
  }
  
  # 计算观察值的频数
  observed <- table(x, y)
  
  # 计算期望值
  row_sums <- rowSums(observed)
  col_sums <- colSums(observed)
  total <- sum(observed)
  expected <- outer(row_sums, col_sums) / total
  
  # 计算卡方统计量
  chisq_stat <- sum((observed - expected)^2 / expected)
  
  return(chisq_stat)
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 示例1：快速计算卡方统计量
x <- c(1, 2, 1, 2, 3, 3, 1, 2, 3)
y <- c(2, 2, 3, 3, 1, 1, 2, 2, 3)

chisq_result <- fast_chisq_test(x, y)
cat("快速计算的卡方统计量：\n", chisq_result, "\n")

# 示例2：比较标准 chisq.test() 的结果
observed_table <- table(x, y)
chisq_test_result <- chisq.test(observed_table)
cat("标准 chisq.test() 的卡方统计量：\n", chisq_test_result$statistic, "\n")

# 清空内存
clean_memory()
```


### 优势

- 相较于标准 `chisq.test()`，省略了不必要的计算（如 p 值），提高了性能。
- 符合题目要求，仅计算卡方统计量，并且程序规范、易于维护。

## 5(P366)
 
你能为以下情况制作一个更快版本的 `table()` 吗？  
输入是两个没有缺失值的整数向量。  
你能用它来加速你的卡方检验吗？

```{r}
# 清空环境变量
rm(list = ls())

# 定义函数：更快的 table() 实现
fast_table <- function(x, y) {
  # 检查输入是否为整数向量
  if (!is.integer(x) || !is.integer(y)) {
    stop("Inputs must be integer vectors.")
  }
  
  # 检查输入是否有缺失值
  if (any(is.na(x)) || any(is.na(y))) {
    stop("Inputs must not contain missing values.")
  }
  
  # 检查输入长度是否匹配
  if (length(x) != length(y)) {
    stop("Inputs must have the same length.")
  }
  
  # 获取唯一值
  x_levels <- unique(x)
  y_levels <- unique(y)
  
  # 初始化频数表
  freq_table <- matrix(0, nrow = length(x_levels), ncol = length(y_levels),
                       dimnames = list(x_levels, y_levels))
  
  # 填充频数表
  for (i in seq_along(x)) {
    freq_table[as.character(x[i]), as.character(y[i])] <- freq_table[as.character(x[i]), as.character(y[i])] + 1
  }
  
  return(freq_table)
}

# 定义函数：更快的卡方检验
fast_chisq_test <- function(x, y) {
  # 调用 fast_table() 构造频数表
  observed <- fast_table(x, y)
  
  # 计算期望值
  row_sums <- rowSums(observed)
  col_sums <- colSums(observed)
  total <- sum(observed)
  expected <- outer(row_sums, col_sums) / total
  
  # 计算卡方统计量
  chisq_stat <- sum((observed - expected)^2 / expected)
  
  return(chisq_stat)
}

# 定义函数：清理内存
clean_memory <- function() {
  rm(list = ls())
  gc()  # 强制垃圾回收
}

# 示例：使用更快的 table 和卡方检验
x <- as.integer(c(1, 2, 1, 2, 3, 3, 1, 2, 3))
y <- as.integer(c(2, 2, 3, 3, 1, 1, 2, 2, 3))

# 快速生成频数表
freq_table <- fast_table(x, y)
cat("快速生成的频数表：\n")
print(freq_table)

# 快速卡方检验
chisq_stat <- fast_chisq_test(x, y)
cat("\n快速卡方检验统计量：\n", chisq_stat, "\n")

# 清空内存
clean_memory()
```

### 优点

- 与标准 `table()` 和 `chisq.test()` 相比，跳过了不必要的操作（如处理因子或非整数数据）。
- 代码简单高效，专注于处理整数向量的卡方检验。
- 符合任务要求，适合高性能需求的应用场景。

## Answer 9

## 问题描述

要实现联合密度函数：

\[
f(x, y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1}, \quad x = 0, 1, \dots, n, \, 0 \leq y \leq 1.
\]

的 Gibbs 采样器。条件分布分别为：
- \(x | y \sim \text{Binomial}(n, y)\)
- \(y | x \sim \text{Beta}(x + a, n - x + b)\)

通过 Rcpp 实现。

## Rcpp 实现
```{r}
# 保存以下代码到 Rcpp 文件中
library(Rcpp)
sourceCpp(code = '
#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix gibbs_sampler(int n_samples, int n, double a, double b, double y_init) {
  NumericMatrix samples(n_samples, 2);
  
  int x = 0;
  double y = y_init;

  for (int i = 0; i < n_samples; ++i) {
    x = R::rbinom(n, y);                  // 从 Binomial(n, y) 中采样 x
    y = R::rbeta(x + a, n - x + b);       // 从 Beta(x + a, n - x + b) 中采样 y

    samples(i, 0) = x;
    samples(i, 1) = y;
  }

  return samples;
}
')
```

## 参数设置
采样器的参数：

- \(n = 10\)：二项分布的样本总数
- \(a = 2\)，\(b = 2\)：Beta 分布的超参数
- 初始值 \(y = 0.5\)
- 总采样数 \(n_{\text{samples}} = 10000\)

```{r}
# 参数设置
n_samples <- 10000
n <- 10
a <- 2
b <- 2
y_init <- 0.5
```

## 运行 Gibbs 采样器

调用 Rcpp 实现的函数并生成样本：

```{r}
# 调用 Gibbs 采样器
samples <- gibbs_sampler(n_samples, n, a, b, y_init)

# 转换为数据框
samples_df <- as.data.frame(samples)
colnames(samples_df) <- c("x", "y")
head(samples_df)
```

## 结果可视化

### \(y\) 的直方图

```{r}
hist(samples_df$y, breaks = 30, main = "Histogram of y", xlab = "y", freq = FALSE)
lines(density(samples_df$y), col = "blue", lwd = 2)
```

### \(x, y\) 的散点图

```{r}
plot(samples_df$x, samples_df$y, pch = 20, col = "darkgreen", 
     main = "Scatter plot of x and y", xlab = "x", ylab = "y")
```

## 总结

通过 Gibbs 采样生成服从目标联合分布的样本。可以观察到：\\
1. \(y\) 的分布接近于 Beta 分布。\\
2. \(x\) 和 \(y\) 之间存在关联性，符合联合密度函数的定义。

### 使用 `qqplot` 比较随机数分布

将 `gibbs_sampler` 函数生成的 \(y\) 样本与 R 中生成的 Beta 分布随机数进行比较。

#### 生成 Beta 分布随机数

使用 R 的 `rbeta` 函数生成相同数量的随机数，参数与 Gibbs 采样的 Beta 条件分布一致。

```{r}
# 从 Gibbs 采样结果提取 y 的样本
gibbs_y <- samples_df$y

# 使用 R 的 rbeta 生成相同数量的随机数
beta_y <- rbeta(n_samples, shape1 = a, shape2 = b)
```

#### 绘制 Q-Q 图

使用 `qqplot` 比较两组随机数是否来自相同分布：

```{r}
# 绘制 Q-Q 图
qqplot(beta_y, gibbs_y, 
       main = "Q-Q Plot: Gibbs vs Beta Random Numbers", 
       xlab = "Quantiles of Beta Distribution (R Function)", 
       ylab = "Quantiles of Gibbs Sampler")
abline(0, 1, col = "red", lwd = 2)  # 添加 y = x 的参考线
```

点大致沿对角线 \(y = x\) 分布，表明两组随机数的分布相似。

## 使用 `microbenchmark` 函数比较Rcpp 实现的 Gibbs 采样器和 R 内置的随机数生成函数的计算时间

### 使用 `microbenchmark` 比较计算时间

比较以下两个任务的时间：
1. **Rcpp 实现的 Gibbs 采样器**：调用 `gibbs_sampler` 函数。
2. **R 内置随机数生成**：直接用 R 的 `rbinom` 和 `rbeta` 函数实现相同的采样逻辑。

#### 代码实现

```{r}
# 加载 microbenchmark 包
library(microbenchmark)

# 使用 R 内置函数实现相同逻辑的采样
gibbs_sampler_r <- function(n_samples, n, a, b, y_init) {
  x <- numeric(n_samples)
  y <- numeric(n_samples)
  y[1] <- y_init
  
  for (i in 1:(n_samples - 1)) {
    # 从 Binomial(n, y) 中采样 x
    x[i] <- rbinom(1, n, y[i])
    # 从 Beta(x + a, n - x + b) 中采样 y
    y[i + 1] <- rbeta(1, x[i] + a, n - x[i] + b)
  }
  data.frame(x = x, y = y)
}

# 比较两个函数的执行时间
benchmark_results <- microbenchmark(
  Rcpp = gibbs_sampler(n_samples, n, a, b, y_init),
  R = gibbs_sampler_r(n_samples, n, a, b, y_init),
  times = 10  # 重复实验 10 次
)

# 打印比较结果
print(benchmark_results)
```

#### 结果解释

运行上述代码后，`microbenchmark` 会返回以下信息：
- 每个函数执行的中位时间 (`median`)。
- 各次实验的最小时间 (`min`) 和最大时间 (`max`)。
- 函数性能的统计分布。

#### 可视化比较

使用 `autoplot` 函数绘制时间比较图：

```{r}
library(ggplot2)
autoplot(benchmark_results)
```

#### 结果分析

 **对比**： 
   - 从图中可以看出，**Rcpp** 实现的函数运行时间更短，分布在 3 毫秒左右，速度明显优于 R 原生实现。
   - 而 **R** 的实现运行时间在 30 毫秒左右，时间更长且波动范围更大。

### 结论

- **性能**：Rcpp 的实现明显比 R 原生实现快得多，这得益于 C++ 的高性能和更低的函数开销。
- **稳定性**：Rcpp 的运行时间分布更集中，说明执行时间更稳定；相比之下，R 的实现波动范围更大。

这种比较结果表明，对于计算密集型任务（如 Gibbs 采样），使用 Rcpp 是一个更高效的选择。


